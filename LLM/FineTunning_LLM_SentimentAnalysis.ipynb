{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24438,
     "status": "ok",
     "timestamp": 1676031747276,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "8lx7aYsIGnR5",
    "outputId": "c270de2b-0e45-42e4-f10b-c8bb36fcd14d"
   },
   "outputs": [],
   "source": [
    "# !pip install keras_bert\n",
    "# !pip install -q keras-bert keras-rectified-adam\n",
    "\n",
    "#Please run these lines on Colab before running the rest of the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **FINE TUNNING A LARGE LANGUAGE MODEL (LLM) FOR SENTIMENT ANALYSIS.**\n",
    "\n",
    "## **Using BERT for sentiment analysis.**\n",
    "\n",
    "\n",
    "<font size=\"3\">During 2018 the Google Research Team released their Large Language Model called BERT. You can find it on the following link: https://github.com/google-research/bert<br><br>\n",
    "\n",
    "<font size=\"3\">BERT stands for Bidirectional Encoder Representation of Transformers. It is a deep learning based unsupervised language representation model. It is the first deeply-bidirectional unsupervised language model. The language models, until BERT, learnt from text sequences in either left-to-right or combined left-to-right and right-to-left contexts. Thus they were either not bidirectional or not bidirectional in all layers.<br><br>\n",
    "    \n",
    "<font size=\"3\">You can use a LLM for many applications. People who work in finance can use it to figure out easily if the words of a CEO or CFO during an earnings call have a positive or negative sentiment. Sentiment analysis involves classifying text into sentiment categories (e.g. positive vs negative). Sentiment analysis allows us to convert complex unstructured data into concise numerical ratings. This is a valuable tool for investors trying to avoid being drowned by the modern firehose of information. The explanations you can find in this notebook are inspired in a Sparkline Capital document called: <a href=\"https://sparklinecapital.files.wordpress.com/2020/11/sparkline-deep-learning.pdf\" target=\"_blank\">Deep Learning in Investing:  Opportunity in Unstructured Data.</a> In that document they showed the following example of sentiment analysis in finance. <br><br>\n",
    "\n",
    "> *â€œYes. So we've never really disclosed beds per door,                  \n",
    "anything like that. What I will say is, we actually just                      \n",
    "completed a pretty big deep dive on this with cohort                    \n",
    "views. And no matter how we cut it, we are continuing                      \n",
    "to see same-store sales increase, which is terrific, and              \n",
    "Q4 was no exception to that.                   \n",
    "So our strength in the marketplace continues to grow.â€    \n",
    "          \n",
    "    Joe Megibow, CEO, Purple Innovation Inc. (Mar 13, 2020). *\n",
    "         \n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<font size=\"3\">\n",
    "<b>Sentiment: Positive</b> \n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<font size=\"3\">If we can build a tool to analyze this kind of information and summarize it, it can be of great help. You can find quickly the overall sentiment of the company's C-level executives and track that information through history.<br><br>\n",
    "\n",
    "<font size=\"3\">In this notebook we are going to get BERT and add a layer to teach it how to perform sentiment analysis. This task is known as **Fine Tunning**. We will show how the model performs on a film reviews dataset. The following notebook should be run on Google Colab using a GPU. Below the credits where we took from most of the implementation ideas.<br><br>\n",
    "    \n",
    "<font size=\"3\">Credits: https://pysnacks.com/machine-learning/bert-text-classification-with-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3355,
     "status": "ok",
     "timestamp": 1676031757735,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "Fl1b6SoNHDlr"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Import libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import codecs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_bert import load_trained_model_from_checkpoint, Tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1316,
     "status": "ok",
     "timestamp": 1676031759045,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "xoNbJc8sHGEj",
    "outputId": "fcd56798-fb76-42e9-a10a-8a8973644add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.9.2\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#Verify Colab is running on a GPU\n",
    "\n",
    "print(\"TensorFlow version:\", tf. __version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 321479,
     "status": "ok",
     "timestamp": 1676032080519,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "iEgVZrNYj0Ph",
    "outputId": "df8e5d0a-6e8b-4d9f-ec47-09b4a51e3eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "#This is a necessary step to read data from you Google Drive\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1676032080520,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "LQqIo5u_kGSF"
   },
   "outputs": [],
   "source": [
    "#Define the path of the folder where you will put BERT files\n",
    "\n",
    "path_folder = '/content/gdrive/MY_OWN_FOLDER'\n",
    "sys.path.append(path_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading BERT.**\n",
    "\n",
    "<font size=\"3\">You should go to: https://github.com/google-research/bert . There, under the Pre-trained models section, you can find different models to use. In this notebook we used BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters. When you download it you get a zip file named uncased_L-12_H-768_A-12.zip. You should unzip the files and put them under MY_OWN_FOLDER as specified before. The 3 files that we need here are: 1) bert_config.json, 2) bert_model.ckpt, 3) vocab.txt <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1676032080521,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "VKvqPWV4HOb-"
   },
   "outputs": [],
   "source": [
    "# Bert Model Constants\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "\n",
    "#Files we need to run the model\n",
    "config_path = path_folder +'/BERT_model/bert_config.json'\n",
    "checkpoint_path = path_folder +'/BERT_model/bert_model.ckpt'\n",
    "vocab_path = path_folder +'/BERT_model/vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading the vocab.txt file**\n",
    "\n",
    "<font size=\"3\">The file vocab.txt that we downladed from the BERT Github site contains more than 30.000 words. Each word is in a separate line in the text file. We can open the vocab.txt file using the codec library as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2185,
     "status": "ok",
     "timestamp": 1676032082695,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "7RL3FgKK4MAY",
    "outputId": "b42397c4-e3bf-4f59-9aeb-512a61d629ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\n",
      "[unused0]\n",
      "[unused1]\n",
      "[unused2]\n"
     ]
    }
   ],
   "source": [
    "#Here we use codec to open the text file. \n",
    "#You can also use import io; io.open(...)\n",
    "\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "  print(reader.read(35)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating the tokenizer**\n",
    "\n",
    "<font size=\"3\">After we open the vocab.txt file we can do the following:\n",
    "1) Read each row in the file and get the text using the function strip <br><br>\n",
    "2) Fill the dictionary token_dict. Each row contains the word of vocab.txt and the number of elements in the dictionary up to that row. <br><br>\n",
    "3) Use the function Tokenizer which we imported from the library keras_bert previously to create our own tokenizer.  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1676032082696,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "TtX2p_xB4O2l"
   },
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()        \n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "tokenizer = Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1676032082696,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "bnwqDuv94Y80",
    "outputId": "b0728631-84b0-49a6-9b44-3325fe2ef005"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " '[unused0]': 1,\n",
       " '[unused1]': 2,\n",
       " '[unused2]': 3,\n",
       " '[unused3]': 4,\n",
       " '[unused4]': 5,\n",
       " '[unused5]': 6,\n",
       " '[unused6]': 7,\n",
       " '[unused7]': 8,\n",
       " '[unused8]': 9,\n",
       " '[unused9]': 10,\n",
       " '[unused10]': 11,\n",
       " '[unused11]': 12,\n",
       " '[unused12]': 13,\n",
       " '[unused13]': 14,\n",
       " '[unused14]': 15,\n",
       " '[unused15]': 16,\n",
       " '[unused16]': 17,\n",
       " '[unused17]': 18,\n",
       " '[unused18]': 19,\n",
       " '[unused19]': 20,\n",
       " '[unused20]': 21,\n",
       " '[unused21]': 22,\n",
       " '[unused22]': 23,\n",
       " '[unused23]': 24,\n",
       " '[unused24]': 25,\n",
       " '[unused25]': 26,\n",
       " '[unused26]': 27,\n",
       " '[unused27]': 28,\n",
       " '[unused28]': 29,\n",
       " '[unused29]': 30,\n",
       " '[unused30]': 31,\n",
       " '[unused31]': 32,\n",
       " '[unused32]': 33,\n",
       " '[unused33]': 34,\n",
       " '[unused34]': 35,\n",
       " '[unused35]': 36,\n",
       " '[unused36]': 37,\n",
       " '[unused37]': 38,\n",
       " '[unused38]': 39,\n",
       " '[unused39]': 40,\n",
       " '[unused40]': 41,\n",
       " '[unused41]': 42,\n",
       " '[unused42]': 43,\n",
       " '[unused43]': 44,\n",
       " '[unused44]': 45,\n",
       " '[unused45]': 46,\n",
       " '[unused46]': 47,\n",
       " '[unused47]': 48,\n",
       " '[unused48]': 49,\n",
       " '[unused49]': 50,\n",
       " '[unused50]': 51,\n",
       " '[unused51]': 52,\n",
       " '[unused52]': 53,\n",
       " '[unused53]': 54,\n",
       " '[unused54]': 55,\n",
       " '[unused55]': 56,\n",
       " '[unused56]': 57,\n",
       " '[unused57]': 58,\n",
       " '[unused58]': 59,\n",
       " '[unused59]': 60,\n",
       " '[unused60]': 61,\n",
       " '[unused61]': 62,\n",
       " '[unused62]': 63,\n",
       " '[unused63]': 64,\n",
       " '[unused64]': 65,\n",
       " '[unused65]': 66,\n",
       " '[unused66]': 67,\n",
       " '[unused67]': 68,\n",
       " '[unused68]': 69,\n",
       " '[unused69]': 70,\n",
       " '[unused70]': 71,\n",
       " '[unused71]': 72,\n",
       " '[unused72]': 73,\n",
       " '[unused73]': 74,\n",
       " '[unused74]': 75,\n",
       " '[unused75]': 76,\n",
       " '[unused76]': 77,\n",
       " '[unused77]': 78,\n",
       " '[unused78]': 79,\n",
       " '[unused79]': 80,\n",
       " '[unused80]': 81,\n",
       " '[unused81]': 82,\n",
       " '[unused82]': 83,\n",
       " '[unused83]': 84,\n",
       " '[unused84]': 85,\n",
       " '[unused85]': 86,\n",
       " '[unused86]': 87,\n",
       " '[unused87]': 88,\n",
       " '[unused88]': 89,\n",
       " '[unused89]': 90,\n",
       " '[unused90]': 91,\n",
       " '[unused91]': 92,\n",
       " '[unused92]': 93,\n",
       " '[unused93]': 94,\n",
       " '[unused94]': 95,\n",
       " '[unused95]': 96,\n",
       " '[unused96]': 97,\n",
       " '[unused97]': 98,\n",
       " '[unused98]': 99,\n",
       " '[UNK]': 100,\n",
       " '[CLS]': 101,\n",
       " '[SEP]': 102,\n",
       " '[MASK]': 103,\n",
       " '[unused99]': 104,\n",
       " '[unused100]': 105,\n",
       " '[unused101]': 106,\n",
       " '[unused102]': 107,\n",
       " '[unused103]': 108,\n",
       " '[unused104]': 109,\n",
       " '[unused105]': 110,\n",
       " '[unused106]': 111,\n",
       " '[unused107]': 112,\n",
       " '[unused108]': 113,\n",
       " '[unused109]': 114,\n",
       " '[unused110]': 115,\n",
       " '[unused111]': 116,\n",
       " '[unused112]': 117,\n",
       " '[unused113]': 118,\n",
       " '[unused114]': 119,\n",
       " '[unused115]': 120,\n",
       " '[unused116]': 121,\n",
       " '[unused117]': 122,\n",
       " '[unused118]': 123,\n",
       " '[unused119]': 124,\n",
       " '[unused120]': 125,\n",
       " '[unused121]': 126,\n",
       " '[unused122]': 127,\n",
       " '[unused123]': 128,\n",
       " '[unused124]': 129,\n",
       " '[unused125]': 130,\n",
       " '[unused126]': 131,\n",
       " '[unused127]': 132,\n",
       " '[unused128]': 133,\n",
       " '[unused129]': 134,\n",
       " '[unused130]': 135,\n",
       " '[unused131]': 136,\n",
       " '[unused132]': 137,\n",
       " '[unused133]': 138,\n",
       " '[unused134]': 139,\n",
       " '[unused135]': 140,\n",
       " '[unused136]': 141,\n",
       " '[unused137]': 142,\n",
       " '[unused138]': 143,\n",
       " '[unused139]': 144,\n",
       " '[unused140]': 145,\n",
       " '[unused141]': 146,\n",
       " '[unused142]': 147,\n",
       " '[unused143]': 148,\n",
       " '[unused144]': 149,\n",
       " '[unused145]': 150,\n",
       " '[unused146]': 151,\n",
       " '[unused147]': 152,\n",
       " '[unused148]': 153,\n",
       " '[unused149]': 154,\n",
       " '[unused150]': 155,\n",
       " '[unused151]': 156,\n",
       " '[unused152]': 157,\n",
       " '[unused153]': 158,\n",
       " '[unused154]': 159,\n",
       " '[unused155]': 160,\n",
       " '[unused156]': 161,\n",
       " '[unused157]': 162,\n",
       " '[unused158]': 163,\n",
       " '[unused159]': 164,\n",
       " '[unused160]': 165,\n",
       " '[unused161]': 166,\n",
       " '[unused162]': 167,\n",
       " '[unused163]': 168,\n",
       " '[unused164]': 169,\n",
       " '[unused165]': 170,\n",
       " '[unused166]': 171,\n",
       " '[unused167]': 172,\n",
       " '[unused168]': 173,\n",
       " '[unused169]': 174,\n",
       " '[unused170]': 175,\n",
       " '[unused171]': 176,\n",
       " '[unused172]': 177,\n",
       " '[unused173]': 178,\n",
       " '[unused174]': 179,\n",
       " '[unused175]': 180,\n",
       " '[unused176]': 181,\n",
       " '[unused177]': 182,\n",
       " '[unused178]': 183,\n",
       " '[unused179]': 184,\n",
       " '[unused180]': 185,\n",
       " '[unused181]': 186,\n",
       " '[unused182]': 187,\n",
       " '[unused183]': 188,\n",
       " '[unused184]': 189,\n",
       " '[unused185]': 190,\n",
       " '[unused186]': 191,\n",
       " '[unused187]': 192,\n",
       " '[unused188]': 193,\n",
       " '[unused189]': 194,\n",
       " '[unused190]': 195,\n",
       " '[unused191]': 196,\n",
       " '[unused192]': 197,\n",
       " '[unused193]': 198,\n",
       " '[unused194]': 199,\n",
       " '[unused195]': 200,\n",
       " '[unused196]': 201,\n",
       " '[unused197]': 202,\n",
       " '[unused198]': 203,\n",
       " '[unused199]': 204,\n",
       " '[unused200]': 205,\n",
       " '[unused201]': 206,\n",
       " '[unused202]': 207,\n",
       " '[unused203]': 208,\n",
       " '[unused204]': 209,\n",
       " '[unused205]': 210,\n",
       " '[unused206]': 211,\n",
       " '[unused207]': 212,\n",
       " '[unused208]': 213,\n",
       " '[unused209]': 214,\n",
       " '[unused210]': 215,\n",
       " '[unused211]': 216,\n",
       " '[unused212]': 217,\n",
       " '[unused213]': 218,\n",
       " '[unused214]': 219,\n",
       " '[unused215]': 220,\n",
       " '[unused216]': 221,\n",
       " '[unused217]': 222,\n",
       " '[unused218]': 223,\n",
       " '[unused219]': 224,\n",
       " '[unused220]': 225,\n",
       " '[unused221]': 226,\n",
       " '[unused222]': 227,\n",
       " '[unused223]': 228,\n",
       " '[unused224]': 229,\n",
       " '[unused225]': 230,\n",
       " '[unused226]': 231,\n",
       " '[unused227]': 232,\n",
       " '[unused228]': 233,\n",
       " '[unused229]': 234,\n",
       " '[unused230]': 235,\n",
       " '[unused231]': 236,\n",
       " '[unused232]': 237,\n",
       " '[unused233]': 238,\n",
       " '[unused234]': 239,\n",
       " '[unused235]': 240,\n",
       " '[unused236]': 241,\n",
       " '[unused237]': 242,\n",
       " '[unused238]': 243,\n",
       " '[unused239]': 244,\n",
       " '[unused240]': 245,\n",
       " '[unused241]': 246,\n",
       " '[unused242]': 247,\n",
       " '[unused243]': 248,\n",
       " '[unused244]': 249,\n",
       " '[unused245]': 250,\n",
       " '[unused246]': 251,\n",
       " '[unused247]': 252,\n",
       " '[unused248]': 253,\n",
       " '[unused249]': 254,\n",
       " '[unused250]': 255,\n",
       " '[unused251]': 256,\n",
       " '[unused252]': 257,\n",
       " '[unused253]': 258,\n",
       " '[unused254]': 259,\n",
       " '[unused255]': 260,\n",
       " '[unused256]': 261,\n",
       " '[unused257]': 262,\n",
       " '[unused258]': 263,\n",
       " '[unused259]': 264,\n",
       " '[unused260]': 265,\n",
       " '[unused261]': 266,\n",
       " '[unused262]': 267,\n",
       " '[unused263]': 268,\n",
       " '[unused264]': 269,\n",
       " '[unused265]': 270,\n",
       " '[unused266]': 271,\n",
       " '[unused267]': 272,\n",
       " '[unused268]': 273,\n",
       " '[unused269]': 274,\n",
       " '[unused270]': 275,\n",
       " '[unused271]': 276,\n",
       " '[unused272]': 277,\n",
       " '[unused273]': 278,\n",
       " '[unused274]': 279,\n",
       " '[unused275]': 280,\n",
       " '[unused276]': 281,\n",
       " '[unused277]': 282,\n",
       " '[unused278]': 283,\n",
       " '[unused279]': 284,\n",
       " '[unused280]': 285,\n",
       " '[unused281]': 286,\n",
       " '[unused282]': 287,\n",
       " '[unused283]': 288,\n",
       " '[unused284]': 289,\n",
       " '[unused285]': 290,\n",
       " '[unused286]': 291,\n",
       " '[unused287]': 292,\n",
       " '[unused288]': 293,\n",
       " '[unused289]': 294,\n",
       " '[unused290]': 295,\n",
       " '[unused291]': 296,\n",
       " '[unused292]': 297,\n",
       " '[unused293]': 298,\n",
       " '[unused294]': 299,\n",
       " '[unused295]': 300,\n",
       " '[unused296]': 301,\n",
       " '[unused297]': 302,\n",
       " '[unused298]': 303,\n",
       " '[unused299]': 304,\n",
       " '[unused300]': 305,\n",
       " '[unused301]': 306,\n",
       " '[unused302]': 307,\n",
       " '[unused303]': 308,\n",
       " '[unused304]': 309,\n",
       " '[unused305]': 310,\n",
       " '[unused306]': 311,\n",
       " '[unused307]': 312,\n",
       " '[unused308]': 313,\n",
       " '[unused309]': 314,\n",
       " '[unused310]': 315,\n",
       " '[unused311]': 316,\n",
       " '[unused312]': 317,\n",
       " '[unused313]': 318,\n",
       " '[unused314]': 319,\n",
       " '[unused315]': 320,\n",
       " '[unused316]': 321,\n",
       " '[unused317]': 322,\n",
       " '[unused318]': 323,\n",
       " '[unused319]': 324,\n",
       " '[unused320]': 325,\n",
       " '[unused321]': 326,\n",
       " '[unused322]': 327,\n",
       " '[unused323]': 328,\n",
       " '[unused324]': 329,\n",
       " '[unused325]': 330,\n",
       " '[unused326]': 331,\n",
       " '[unused327]': 332,\n",
       " '[unused328]': 333,\n",
       " '[unused329]': 334,\n",
       " '[unused330]': 335,\n",
       " '[unused331]': 336,\n",
       " '[unused332]': 337,\n",
       " '[unused333]': 338,\n",
       " '[unused334]': 339,\n",
       " '[unused335]': 340,\n",
       " '[unused336]': 341,\n",
       " '[unused337]': 342,\n",
       " '[unused338]': 343,\n",
       " '[unused339]': 344,\n",
       " '[unused340]': 345,\n",
       " '[unused341]': 346,\n",
       " '[unused342]': 347,\n",
       " '[unused343]': 348,\n",
       " '[unused344]': 349,\n",
       " '[unused345]': 350,\n",
       " '[unused346]': 351,\n",
       " '[unused347]': 352,\n",
       " '[unused348]': 353,\n",
       " '[unused349]': 354,\n",
       " '[unused350]': 355,\n",
       " '[unused351]': 356,\n",
       " '[unused352]': 357,\n",
       " '[unused353]': 358,\n",
       " '[unused354]': 359,\n",
       " '[unused355]': 360,\n",
       " '[unused356]': 361,\n",
       " '[unused357]': 362,\n",
       " '[unused358]': 363,\n",
       " '[unused359]': 364,\n",
       " '[unused360]': 365,\n",
       " '[unused361]': 366,\n",
       " '[unused362]': 367,\n",
       " '[unused363]': 368,\n",
       " '[unused364]': 369,\n",
       " '[unused365]': 370,\n",
       " '[unused366]': 371,\n",
       " '[unused367]': 372,\n",
       " '[unused368]': 373,\n",
       " '[unused369]': 374,\n",
       " '[unused370]': 375,\n",
       " '[unused371]': 376,\n",
       " '[unused372]': 377,\n",
       " '[unused373]': 378,\n",
       " '[unused374]': 379,\n",
       " '[unused375]': 380,\n",
       " '[unused376]': 381,\n",
       " '[unused377]': 382,\n",
       " '[unused378]': 383,\n",
       " '[unused379]': 384,\n",
       " '[unused380]': 385,\n",
       " '[unused381]': 386,\n",
       " '[unused382]': 387,\n",
       " '[unused383]': 388,\n",
       " '[unused384]': 389,\n",
       " '[unused385]': 390,\n",
       " '[unused386]': 391,\n",
       " '[unused387]': 392,\n",
       " '[unused388]': 393,\n",
       " '[unused389]': 394,\n",
       " '[unused390]': 395,\n",
       " '[unused391]': 396,\n",
       " '[unused392]': 397,\n",
       " '[unused393]': 398,\n",
       " '[unused394]': 399,\n",
       " '[unused395]': 400,\n",
       " '[unused396]': 401,\n",
       " '[unused397]': 402,\n",
       " '[unused398]': 403,\n",
       " '[unused399]': 404,\n",
       " '[unused400]': 405,\n",
       " '[unused401]': 406,\n",
       " '[unused402]': 407,\n",
       " '[unused403]': 408,\n",
       " '[unused404]': 409,\n",
       " '[unused405]': 410,\n",
       " '[unused406]': 411,\n",
       " '[unused407]': 412,\n",
       " '[unused408]': 413,\n",
       " '[unused409]': 414,\n",
       " '[unused410]': 415,\n",
       " '[unused411]': 416,\n",
       " '[unused412]': 417,\n",
       " '[unused413]': 418,\n",
       " '[unused414]': 419,\n",
       " '[unused415]': 420,\n",
       " '[unused416]': 421,\n",
       " '[unused417]': 422,\n",
       " '[unused418]': 423,\n",
       " '[unused419]': 424,\n",
       " '[unused420]': 425,\n",
       " '[unused421]': 426,\n",
       " '[unused422]': 427,\n",
       " '[unused423]': 428,\n",
       " '[unused424]': 429,\n",
       " '[unused425]': 430,\n",
       " '[unused426]': 431,\n",
       " '[unused427]': 432,\n",
       " '[unused428]': 433,\n",
       " '[unused429]': 434,\n",
       " '[unused430]': 435,\n",
       " '[unused431]': 436,\n",
       " '[unused432]': 437,\n",
       " '[unused433]': 438,\n",
       " '[unused434]': 439,\n",
       " '[unused435]': 440,\n",
       " '[unused436]': 441,\n",
       " '[unused437]': 442,\n",
       " '[unused438]': 443,\n",
       " '[unused439]': 444,\n",
       " '[unused440]': 445,\n",
       " '[unused441]': 446,\n",
       " '[unused442]': 447,\n",
       " '[unused443]': 448,\n",
       " '[unused444]': 449,\n",
       " '[unused445]': 450,\n",
       " '[unused446]': 451,\n",
       " '[unused447]': 452,\n",
       " '[unused448]': 453,\n",
       " '[unused449]': 454,\n",
       " '[unused450]': 455,\n",
       " '[unused451]': 456,\n",
       " '[unused452]': 457,\n",
       " '[unused453]': 458,\n",
       " '[unused454]': 459,\n",
       " '[unused455]': 460,\n",
       " '[unused456]': 461,\n",
       " '[unused457]': 462,\n",
       " '[unused458]': 463,\n",
       " '[unused459]': 464,\n",
       " '[unused460]': 465,\n",
       " '[unused461]': 466,\n",
       " '[unused462]': 467,\n",
       " '[unused463]': 468,\n",
       " '[unused464]': 469,\n",
       " '[unused465]': 470,\n",
       " '[unused466]': 471,\n",
       " '[unused467]': 472,\n",
       " '[unused468]': 473,\n",
       " '[unused469]': 474,\n",
       " '[unused470]': 475,\n",
       " '[unused471]': 476,\n",
       " '[unused472]': 477,\n",
       " '[unused473]': 478,\n",
       " '[unused474]': 479,\n",
       " '[unused475]': 480,\n",
       " '[unused476]': 481,\n",
       " '[unused477]': 482,\n",
       " '[unused478]': 483,\n",
       " '[unused479]': 484,\n",
       " '[unused480]': 485,\n",
       " '[unused481]': 486,\n",
       " '[unused482]': 487,\n",
       " '[unused483]': 488,\n",
       " '[unused484]': 489,\n",
       " '[unused485]': 490,\n",
       " '[unused486]': 491,\n",
       " '[unused487]': 492,\n",
       " '[unused488]': 493,\n",
       " '[unused489]': 494,\n",
       " '[unused490]': 495,\n",
       " '[unused491]': 496,\n",
       " '[unused492]': 497,\n",
       " '[unused493]': 498,\n",
       " '[unused494]': 499,\n",
       " '[unused495]': 500,\n",
       " '[unused496]': 501,\n",
       " '[unused497]': 502,\n",
       " '[unused498]': 503,\n",
       " '[unused499]': 504,\n",
       " '[unused500]': 505,\n",
       " '[unused501]': 506,\n",
       " '[unused502]': 507,\n",
       " '[unused503]': 508,\n",
       " '[unused504]': 509,\n",
       " '[unused505]': 510,\n",
       " '[unused506]': 511,\n",
       " '[unused507]': 512,\n",
       " '[unused508]': 513,\n",
       " '[unused509]': 514,\n",
       " '[unused510]': 515,\n",
       " '[unused511]': 516,\n",
       " '[unused512]': 517,\n",
       " '[unused513]': 518,\n",
       " '[unused514]': 519,\n",
       " '[unused515]': 520,\n",
       " '[unused516]': 521,\n",
       " '[unused517]': 522,\n",
       " '[unused518]': 523,\n",
       " '[unused519]': 524,\n",
       " '[unused520]': 525,\n",
       " '[unused521]': 526,\n",
       " '[unused522]': 527,\n",
       " '[unused523]': 528,\n",
       " '[unused524]': 529,\n",
       " '[unused525]': 530,\n",
       " '[unused526]': 531,\n",
       " '[unused527]': 532,\n",
       " '[unused528]': 533,\n",
       " '[unused529]': 534,\n",
       " '[unused530]': 535,\n",
       " '[unused531]': 536,\n",
       " '[unused532]': 537,\n",
       " '[unused533]': 538,\n",
       " '[unused534]': 539,\n",
       " '[unused535]': 540,\n",
       " '[unused536]': 541,\n",
       " '[unused537]': 542,\n",
       " '[unused538]': 543,\n",
       " '[unused539]': 544,\n",
       " '[unused540]': 545,\n",
       " '[unused541]': 546,\n",
       " '[unused542]': 547,\n",
       " '[unused543]': 548,\n",
       " '[unused544]': 549,\n",
       " '[unused545]': 550,\n",
       " '[unused546]': 551,\n",
       " '[unused547]': 552,\n",
       " '[unused548]': 553,\n",
       " '[unused549]': 554,\n",
       " '[unused550]': 555,\n",
       " '[unused551]': 556,\n",
       " '[unused552]': 557,\n",
       " '[unused553]': 558,\n",
       " '[unused554]': 559,\n",
       " '[unused555]': 560,\n",
       " '[unused556]': 561,\n",
       " '[unused557]': 562,\n",
       " '[unused558]': 563,\n",
       " '[unused559]': 564,\n",
       " '[unused560]': 565,\n",
       " '[unused561]': 566,\n",
       " '[unused562]': 567,\n",
       " '[unused563]': 568,\n",
       " '[unused564]': 569,\n",
       " '[unused565]': 570,\n",
       " '[unused566]': 571,\n",
       " '[unused567]': 572,\n",
       " '[unused568]': 573,\n",
       " '[unused569]': 574,\n",
       " '[unused570]': 575,\n",
       " '[unused571]': 576,\n",
       " '[unused572]': 577,\n",
       " '[unused573]': 578,\n",
       " '[unused574]': 579,\n",
       " '[unused575]': 580,\n",
       " '[unused576]': 581,\n",
       " '[unused577]': 582,\n",
       " '[unused578]': 583,\n",
       " '[unused579]': 584,\n",
       " '[unused580]': 585,\n",
       " '[unused581]': 586,\n",
       " '[unused582]': 587,\n",
       " '[unused583]': 588,\n",
       " '[unused584]': 589,\n",
       " '[unused585]': 590,\n",
       " '[unused586]': 591,\n",
       " '[unused587]': 592,\n",
       " '[unused588]': 593,\n",
       " '[unused589]': 594,\n",
       " '[unused590]': 595,\n",
       " '[unused591]': 596,\n",
       " '[unused592]': 597,\n",
       " '[unused593]': 598,\n",
       " '[unused594]': 599,\n",
       " '[unused595]': 600,\n",
       " '[unused596]': 601,\n",
       " '[unused597]': 602,\n",
       " '[unused598]': 603,\n",
       " '[unused599]': 604,\n",
       " '[unused600]': 605,\n",
       " '[unused601]': 606,\n",
       " '[unused602]': 607,\n",
       " '[unused603]': 608,\n",
       " '[unused604]': 609,\n",
       " '[unused605]': 610,\n",
       " '[unused606]': 611,\n",
       " '[unused607]': 612,\n",
       " '[unused608]': 613,\n",
       " '[unused609]': 614,\n",
       " '[unused610]': 615,\n",
       " '[unused611]': 616,\n",
       " '[unused612]': 617,\n",
       " '[unused613]': 618,\n",
       " '[unused614]': 619,\n",
       " '[unused615]': 620,\n",
       " '[unused616]': 621,\n",
       " '[unused617]': 622,\n",
       " '[unused618]': 623,\n",
       " '[unused619]': 624,\n",
       " '[unused620]': 625,\n",
       " '[unused621]': 626,\n",
       " '[unused622]': 627,\n",
       " '[unused623]': 628,\n",
       " '[unused624]': 629,\n",
       " '[unused625]': 630,\n",
       " '[unused626]': 631,\n",
       " '[unused627]': 632,\n",
       " '[unused628]': 633,\n",
       " '[unused629]': 634,\n",
       " '[unused630]': 635,\n",
       " '[unused631]': 636,\n",
       " '[unused632]': 637,\n",
       " '[unused633]': 638,\n",
       " '[unused634]': 639,\n",
       " '[unused635]': 640,\n",
       " '[unused636]': 641,\n",
       " '[unused637]': 642,\n",
       " '[unused638]': 643,\n",
       " '[unused639]': 644,\n",
       " '[unused640]': 645,\n",
       " '[unused641]': 646,\n",
       " '[unused642]': 647,\n",
       " '[unused643]': 648,\n",
       " '[unused644]': 649,\n",
       " '[unused645]': 650,\n",
       " '[unused646]': 651,\n",
       " '[unused647]': 652,\n",
       " '[unused648]': 653,\n",
       " '[unused649]': 654,\n",
       " '[unused650]': 655,\n",
       " '[unused651]': 656,\n",
       " '[unused652]': 657,\n",
       " '[unused653]': 658,\n",
       " '[unused654]': 659,\n",
       " '[unused655]': 660,\n",
       " '[unused656]': 661,\n",
       " '[unused657]': 662,\n",
       " '[unused658]': 663,\n",
       " '[unused659]': 664,\n",
       " '[unused660]': 665,\n",
       " '[unused661]': 666,\n",
       " '[unused662]': 667,\n",
       " '[unused663]': 668,\n",
       " '[unused664]': 669,\n",
       " '[unused665]': 670,\n",
       " '[unused666]': 671,\n",
       " '[unused667]': 672,\n",
       " '[unused668]': 673,\n",
       " '[unused669]': 674,\n",
       " '[unused670]': 675,\n",
       " '[unused671]': 676,\n",
       " '[unused672]': 677,\n",
       " '[unused673]': 678,\n",
       " '[unused674]': 679,\n",
       " '[unused675]': 680,\n",
       " '[unused676]': 681,\n",
       " '[unused677]': 682,\n",
       " '[unused678]': 683,\n",
       " '[unused679]': 684,\n",
       " '[unused680]': 685,\n",
       " '[unused681]': 686,\n",
       " '[unused682]': 687,\n",
       " '[unused683]': 688,\n",
       " '[unused684]': 689,\n",
       " '[unused685]': 690,\n",
       " '[unused686]': 691,\n",
       " '[unused687]': 692,\n",
       " '[unused688]': 693,\n",
       " '[unused689]': 694,\n",
       " '[unused690]': 695,\n",
       " '[unused691]': 696,\n",
       " '[unused692]': 697,\n",
       " '[unused693]': 698,\n",
       " '[unused694]': 699,\n",
       " '[unused695]': 700,\n",
       " '[unused696]': 701,\n",
       " '[unused697]': 702,\n",
       " '[unused698]': 703,\n",
       " '[unused699]': 704,\n",
       " '[unused700]': 705,\n",
       " '[unused701]': 706,\n",
       " '[unused702]': 707,\n",
       " '[unused703]': 708,\n",
       " '[unused704]': 709,\n",
       " '[unused705]': 710,\n",
       " '[unused706]': 711,\n",
       " '[unused707]': 712,\n",
       " '[unused708]': 713,\n",
       " '[unused709]': 714,\n",
       " '[unused710]': 715,\n",
       " '[unused711]': 716,\n",
       " '[unused712]': 717,\n",
       " '[unused713]': 718,\n",
       " '[unused714]': 719,\n",
       " '[unused715]': 720,\n",
       " '[unused716]': 721,\n",
       " '[unused717]': 722,\n",
       " '[unused718]': 723,\n",
       " '[unused719]': 724,\n",
       " '[unused720]': 725,\n",
       " '[unused721]': 726,\n",
       " '[unused722]': 727,\n",
       " '[unused723]': 728,\n",
       " '[unused724]': 729,\n",
       " '[unused725]': 730,\n",
       " '[unused726]': 731,\n",
       " '[unused727]': 732,\n",
       " '[unused728]': 733,\n",
       " '[unused729]': 734,\n",
       " '[unused730]': 735,\n",
       " '[unused731]': 736,\n",
       " '[unused732]': 737,\n",
       " '[unused733]': 738,\n",
       " '[unused734]': 739,\n",
       " '[unused735]': 740,\n",
       " '[unused736]': 741,\n",
       " '[unused737]': 742,\n",
       " '[unused738]': 743,\n",
       " '[unused739]': 744,\n",
       " '[unused740]': 745,\n",
       " '[unused741]': 746,\n",
       " '[unused742]': 747,\n",
       " '[unused743]': 748,\n",
       " '[unused744]': 749,\n",
       " '[unused745]': 750,\n",
       " '[unused746]': 751,\n",
       " '[unused747]': 752,\n",
       " '[unused748]': 753,\n",
       " '[unused749]': 754,\n",
       " '[unused750]': 755,\n",
       " '[unused751]': 756,\n",
       " '[unused752]': 757,\n",
       " '[unused753]': 758,\n",
       " '[unused754]': 759,\n",
       " '[unused755]': 760,\n",
       " '[unused756]': 761,\n",
       " '[unused757]': 762,\n",
       " '[unused758]': 763,\n",
       " '[unused759]': 764,\n",
       " '[unused760]': 765,\n",
       " '[unused761]': 766,\n",
       " '[unused762]': 767,\n",
       " '[unused763]': 768,\n",
       " '[unused764]': 769,\n",
       " '[unused765]': 770,\n",
       " '[unused766]': 771,\n",
       " '[unused767]': 772,\n",
       " '[unused768]': 773,\n",
       " '[unused769]': 774,\n",
       " '[unused770]': 775,\n",
       " '[unused771]': 776,\n",
       " '[unused772]': 777,\n",
       " '[unused773]': 778,\n",
       " '[unused774]': 779,\n",
       " '[unused775]': 780,\n",
       " '[unused776]': 781,\n",
       " '[unused777]': 782,\n",
       " '[unused778]': 783,\n",
       " '[unused779]': 784,\n",
       " '[unused780]': 785,\n",
       " '[unused781]': 786,\n",
       " '[unused782]': 787,\n",
       " '[unused783]': 788,\n",
       " '[unused784]': 789,\n",
       " '[unused785]': 790,\n",
       " '[unused786]': 791,\n",
       " '[unused787]': 792,\n",
       " '[unused788]': 793,\n",
       " '[unused789]': 794,\n",
       " '[unused790]': 795,\n",
       " '[unused791]': 796,\n",
       " '[unused792]': 797,\n",
       " '[unused793]': 798,\n",
       " '[unused794]': 799,\n",
       " '[unused795]': 800,\n",
       " '[unused796]': 801,\n",
       " '[unused797]': 802,\n",
       " '[unused798]': 803,\n",
       " '[unused799]': 804,\n",
       " '[unused800]': 805,\n",
       " '[unused801]': 806,\n",
       " '[unused802]': 807,\n",
       " '[unused803]': 808,\n",
       " '[unused804]': 809,\n",
       " '[unused805]': 810,\n",
       " '[unused806]': 811,\n",
       " '[unused807]': 812,\n",
       " '[unused808]': 813,\n",
       " '[unused809]': 814,\n",
       " '[unused810]': 815,\n",
       " '[unused811]': 816,\n",
       " '[unused812]': 817,\n",
       " '[unused813]': 818,\n",
       " '[unused814]': 819,\n",
       " '[unused815]': 820,\n",
       " '[unused816]': 821,\n",
       " '[unused817]': 822,\n",
       " '[unused818]': 823,\n",
       " '[unused819]': 824,\n",
       " '[unused820]': 825,\n",
       " '[unused821]': 826,\n",
       " '[unused822]': 827,\n",
       " '[unused823]': 828,\n",
       " '[unused824]': 829,\n",
       " '[unused825]': 830,\n",
       " '[unused826]': 831,\n",
       " '[unused827]': 832,\n",
       " '[unused828]': 833,\n",
       " '[unused829]': 834,\n",
       " '[unused830]': 835,\n",
       " '[unused831]': 836,\n",
       " '[unused832]': 837,\n",
       " '[unused833]': 838,\n",
       " '[unused834]': 839,\n",
       " '[unused835]': 840,\n",
       " '[unused836]': 841,\n",
       " '[unused837]': 842,\n",
       " '[unused838]': 843,\n",
       " '[unused839]': 844,\n",
       " '[unused840]': 845,\n",
       " '[unused841]': 846,\n",
       " '[unused842]': 847,\n",
       " '[unused843]': 848,\n",
       " '[unused844]': 849,\n",
       " '[unused845]': 850,\n",
       " '[unused846]': 851,\n",
       " '[unused847]': 852,\n",
       " '[unused848]': 853,\n",
       " '[unused849]': 854,\n",
       " '[unused850]': 855,\n",
       " '[unused851]': 856,\n",
       " '[unused852]': 857,\n",
       " '[unused853]': 858,\n",
       " '[unused854]': 859,\n",
       " '[unused855]': 860,\n",
       " '[unused856]': 861,\n",
       " '[unused857]': 862,\n",
       " '[unused858]': 863,\n",
       " '[unused859]': 864,\n",
       " '[unused860]': 865,\n",
       " '[unused861]': 866,\n",
       " '[unused862]': 867,\n",
       " '[unused863]': 868,\n",
       " '[unused864]': 869,\n",
       " '[unused865]': 870,\n",
       " '[unused866]': 871,\n",
       " '[unused867]': 872,\n",
       " '[unused868]': 873,\n",
       " '[unused869]': 874,\n",
       " '[unused870]': 875,\n",
       " '[unused871]': 876,\n",
       " '[unused872]': 877,\n",
       " '[unused873]': 878,\n",
       " '[unused874]': 879,\n",
       " '[unused875]': 880,\n",
       " '[unused876]': 881,\n",
       " '[unused877]': 882,\n",
       " '[unused878]': 883,\n",
       " '[unused879]': 884,\n",
       " '[unused880]': 885,\n",
       " '[unused881]': 886,\n",
       " '[unused882]': 887,\n",
       " '[unused883]': 888,\n",
       " '[unused884]': 889,\n",
       " '[unused885]': 890,\n",
       " '[unused886]': 891,\n",
       " '[unused887]': 892,\n",
       " '[unused888]': 893,\n",
       " '[unused889]': 894,\n",
       " '[unused890]': 895,\n",
       " '[unused891]': 896,\n",
       " '[unused892]': 897,\n",
       " '[unused893]': 898,\n",
       " '[unused894]': 899,\n",
       " '[unused895]': 900,\n",
       " '[unused896]': 901,\n",
       " '[unused897]': 902,\n",
       " '[unused898]': 903,\n",
       " '[unused899]': 904,\n",
       " '[unused900]': 905,\n",
       " '[unused901]': 906,\n",
       " '[unused902]': 907,\n",
       " '[unused903]': 908,\n",
       " '[unused904]': 909,\n",
       " '[unused905]': 910,\n",
       " '[unused906]': 911,\n",
       " '[unused907]': 912,\n",
       " '[unused908]': 913,\n",
       " '[unused909]': 914,\n",
       " '[unused910]': 915,\n",
       " '[unused911]': 916,\n",
       " '[unused912]': 917,\n",
       " '[unused913]': 918,\n",
       " '[unused914]': 919,\n",
       " '[unused915]': 920,\n",
       " '[unused916]': 921,\n",
       " '[unused917]': 922,\n",
       " '[unused918]': 923,\n",
       " '[unused919]': 924,\n",
       " '[unused920]': 925,\n",
       " '[unused921]': 926,\n",
       " '[unused922]': 927,\n",
       " '[unused923]': 928,\n",
       " '[unused924]': 929,\n",
       " '[unused925]': 930,\n",
       " '[unused926]': 931,\n",
       " '[unused927]': 932,\n",
       " '[unused928]': 933,\n",
       " '[unused929]': 934,\n",
       " '[unused930]': 935,\n",
       " '[unused931]': 936,\n",
       " '[unused932]': 937,\n",
       " '[unused933]': 938,\n",
       " '[unused934]': 939,\n",
       " '[unused935]': 940,\n",
       " '[unused936]': 941,\n",
       " '[unused937]': 942,\n",
       " '[unused938]': 943,\n",
       " '[unused939]': 944,\n",
       " '[unused940]': 945,\n",
       " '[unused941]': 946,\n",
       " '[unused942]': 947,\n",
       " '[unused943]': 948,\n",
       " '[unused944]': 949,\n",
       " '[unused945]': 950,\n",
       " '[unused946]': 951,\n",
       " '[unused947]': 952,\n",
       " '[unused948]': 953,\n",
       " '[unused949]': 954,\n",
       " '[unused950]': 955,\n",
       " '[unused951]': 956,\n",
       " '[unused952]': 957,\n",
       " '[unused953]': 958,\n",
       " '[unused954]': 959,\n",
       " '[unused955]': 960,\n",
       " '[unused956]': 961,\n",
       " '[unused957]': 962,\n",
       " '[unused958]': 963,\n",
       " '[unused959]': 964,\n",
       " '[unused960]': 965,\n",
       " '[unused961]': 966,\n",
       " '[unused962]': 967,\n",
       " '[unused963]': 968,\n",
       " '[unused964]': 969,\n",
       " '[unused965]': 970,\n",
       " '[unused966]': 971,\n",
       " '[unused967]': 972,\n",
       " '[unused968]': 973,\n",
       " '[unused969]': 974,\n",
       " '[unused970]': 975,\n",
       " '[unused971]': 976,\n",
       " '[unused972]': 977,\n",
       " '[unused973]': 978,\n",
       " '[unused974]': 979,\n",
       " '[unused975]': 980,\n",
       " '[unused976]': 981,\n",
       " '[unused977]': 982,\n",
       " '[unused978]': 983,\n",
       " '[unused979]': 984,\n",
       " '[unused980]': 985,\n",
       " '[unused981]': 986,\n",
       " '[unused982]': 987,\n",
       " '[unused983]': 988,\n",
       " '[unused984]': 989,\n",
       " '[unused985]': 990,\n",
       " '[unused986]': 991,\n",
       " '[unused987]': 992,\n",
       " '[unused988]': 993,\n",
       " '[unused989]': 994,\n",
       " '[unused990]': 995,\n",
       " '[unused991]': 996,\n",
       " '[unused992]': 997,\n",
       " '[unused993]': 998,\n",
       " '!': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Inspection of token_dict\n",
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1676032082696,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "HY7Qls7i4b0l",
    "outputId": "f44197d7-bab3-4644-ea12-0d172bd812ef",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'wait',\n",
       " 'for',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'next',\n",
       " 'part',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is what the tokenizer returns using an example of a short movie review. \n",
    "tokenizer.tokenize(\"Can't wait for it's next part!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Downloading Large Movie Review Dataset.**\n",
    "\n",
    "<font size=\"3\">From Stanford University we will download the Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. They provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. Here we use the function tf.keras.utils.get_file . In the following link you can see what this function does https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29751,
     "status": "ok",
     "timestamp": 1676032112442,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "0KawJcfYIioz",
    "outputId": "ab89b3f9-3b1d-414b-8211-d22c3eab3a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84125825/84125825 [==============================] - 8s 0us/step\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "    fname=\"aclImdb.tar.gz\", \n",
    "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "    extract=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1676032112442,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "juHeQ6MDuhip",
    "outputId": "9821ea58-87a8-4189-a669-7bd27c73d2d3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/root/.keras/datasets/aclImdb.tar.gz'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1676032112443,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "TUSn1e3_miYa",
    "outputId": "4c599799-d8cc-4357-a896-c8cea0864efc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aclImdb', 'aclImdb.tar.gz']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Python method listdir() returns a list containing the names of the entries in the directory given by path. \n",
    "#The list is in arbitrary order\n",
    "os.listdir('/root/.keras/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1676032112443,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "r988BrDumkFE",
    "outputId": "efb32072-1efd-4dd6-a5f0-34b4f08a3274"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'README', 'imdbEr.txt', 'test', 'imdb.vocab']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We have train and test folders\n",
    "os.listdir('/root/.keras/datasets/aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1676032112443,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "x-ei_tBxmpvm",
    "outputId": "39bf90ac-5d0d-4320-aa0b-9d461f816729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['urls_neg.txt',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'unsup',\n",
       " 'neg',\n",
       " 'labeledBow.feat',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In train and test folders we also have pos and neg folders.\n",
    "#Pos stands for positive and neg for negative. There we have the film reviews that are positive or negative\n",
    "os.listdir('/root/.keras/datasets/aclImdb/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1676032112444,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "CECUpiuBmxq6",
    "outputId": "204dbe96-d33a-4947-cd4a-993c4ca3673e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1528_1.txt',\n",
       " '6684_1.txt',\n",
       " '6173_1.txt',\n",
       " '6371_4.txt',\n",
       " '9009_1.txt',\n",
       " '11580_2.txt',\n",
       " '12161_3.txt',\n",
       " '10844_1.txt',\n",
       " '221_4.txt',\n",
       " '9018_3.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is an example of the reviews in each folder\n",
    "os.listdir('/root/.keras/datasets/aclImdb/train/neg')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Organizing the data from movie reviews**\n",
    "\n",
    "<font size=\"3\">Here we will create a function to get all the positive and negative reviews and tokenize them. With that information in hand we can feed our deep neural network and perform a sentiment analysis. <br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1676032112444,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "ccdIdOtut27Z"
   },
   "outputs": [],
   "source": [
    "#Define the train and test paths\n",
    "train_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'train')\n",
    "test_path = os.path.join(os.path.dirname(dataset), 'aclImdb', 'test')\n",
    "\n",
    "#Define the negative and positive labels\n",
    "tagset = [('neg', 0), ('pos', 1)]\n",
    "id_to_labels = {0: 'negative', 1: 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1676032112444,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "4y5-xzT1t6eB",
    "outputId": "412116ac-a45a-46b6-957f-d6a2554c8629"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.keras/datasets/aclImdb/train/neg\n",
      "----------------------------------------\n",
      "0\n",
      "/root/.keras/datasets/aclImdb/train/pos\n",
      "----------------------------------------\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for folder, sentiment in tagset:\n",
    "  folder = os.path.join(train_path, folder)\n",
    "  print(folder)\n",
    "  print(\"----------------------------------------\")\n",
    "  print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1676032112445,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "7-9xltxb5tr4",
    "outputId": "300b1820-6a66-46b1-8f37-1e42484ff648"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Soylent Green is a classic. I have been waiting for someone to re-do it.They seem to be remaking sci-fi classics these days (i.e. War of the Worlds)and I am hoping some director/producer will re-do Soylent Green. With todays computer animation and technology, it would have the potential to be a great picture. Anti-Utopian films may not be that far-fetched. The human race breeds like roaches with no outside influence to curtail it. We, as humans, have the option of putting the kibosh on the procreation of lesser species if they get out of hand, but there's nothing to control human breeding except for ourselves. Despite all the diseases, wars, abortions, birth control, etc. the human race still multiplies like bacteria in a petri dish. Classic Malthusian economics states that any species, including humans, will multiply beyond their means of subsistence. 6 billion and growing....that's obscene.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open a particular review and read its text\n",
    "with open(os.path.join(folder, os.listdir(folder)[0]), 'r',encoding=\"utf-8\") as reader:\n",
    "  text = reader.read()\n",
    "  \n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1676032112445,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "dJndarslay8W",
    "outputId": "efa435de-db31-495c-b77c-a4c3d4ce0f9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#There are 12500 positive reviews in the trainning set \n",
    "len(os.listdir(folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1676032112445,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "zt-lx4Q_bwur",
    "outputId": "91523498-5c78-4ac9-b53e-e5b7ea10a29a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'soy',\n",
       " '##lent',\n",
       " 'green',\n",
       " 'is',\n",
       " 'a',\n",
       " 'classic',\n",
       " '.',\n",
       " 'i',\n",
       " 'have',\n",
       " 'been',\n",
       " 'waiting',\n",
       " 'for',\n",
       " 'someone',\n",
       " 'to',\n",
       " 're',\n",
       " '-',\n",
       " 'do',\n",
       " 'it',\n",
       " '.',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'be',\n",
       " 're',\n",
       " '##making',\n",
       " 'sci',\n",
       " '-',\n",
       " 'fi',\n",
       " 'classics',\n",
       " 'these',\n",
       " 'days',\n",
       " '(',\n",
       " 'i',\n",
       " '.',\n",
       " 'e',\n",
       " '.',\n",
       " 'war',\n",
       " 'of',\n",
       " 'the',\n",
       " 'worlds',\n",
       " ')',\n",
       " 'and',\n",
       " 'i',\n",
       " 'am',\n",
       " 'hoping',\n",
       " 'some',\n",
       " 'director',\n",
       " '/',\n",
       " 'producer',\n",
       " 'will',\n",
       " 're',\n",
       " '-',\n",
       " 'do',\n",
       " 'soy',\n",
       " '##lent',\n",
       " 'green',\n",
       " '.',\n",
       " 'with',\n",
       " 'today',\n",
       " '##s',\n",
       " 'computer',\n",
       " 'animation',\n",
       " 'and',\n",
       " 'technology',\n",
       " ',',\n",
       " 'it',\n",
       " 'would',\n",
       " 'have',\n",
       " 'the',\n",
       " 'potential',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'great',\n",
       " 'picture',\n",
       " '.',\n",
       " 'anti',\n",
       " '-',\n",
       " 'utopia',\n",
       " '##n',\n",
       " 'films',\n",
       " 'may',\n",
       " 'not',\n",
       " 'be',\n",
       " 'that',\n",
       " 'far',\n",
       " '-',\n",
       " 'fetch',\n",
       " '##ed',\n",
       " '.',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " 'breeds',\n",
       " 'like',\n",
       " 'roach',\n",
       " '##es',\n",
       " 'with',\n",
       " 'no',\n",
       " 'outside',\n",
       " 'influence',\n",
       " 'to',\n",
       " 'curt',\n",
       " '##ail',\n",
       " 'it',\n",
       " '.',\n",
       " 'we',\n",
       " ',',\n",
       " 'as',\n",
       " 'humans',\n",
       " ',',\n",
       " 'have',\n",
       " 'the',\n",
       " 'option',\n",
       " 'of',\n",
       " 'putting',\n",
       " 'the',\n",
       " 'ki',\n",
       " '##bos',\n",
       " '##h',\n",
       " 'on',\n",
       " 'the',\n",
       " 'pro',\n",
       " '##cre',\n",
       " '##ation',\n",
       " 'of',\n",
       " 'lesser',\n",
       " 'species',\n",
       " 'if',\n",
       " 'they',\n",
       " 'get',\n",
       " 'out',\n",
       " 'of',\n",
       " 'hand',\n",
       " ',',\n",
       " 'but',\n",
       " 'there',\n",
       " \"'\",\n",
       " 's',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'control',\n",
       " 'human',\n",
       " 'breeding',\n",
       " 'except',\n",
       " 'for',\n",
       " 'ourselves',\n",
       " '.',\n",
       " 'despite',\n",
       " 'all',\n",
       " 'the',\n",
       " 'diseases',\n",
       " ',',\n",
       " 'wars',\n",
       " ',',\n",
       " 'abortion',\n",
       " '##s',\n",
       " ',',\n",
       " 'birth',\n",
       " 'control',\n",
       " ',',\n",
       " 'etc',\n",
       " '.',\n",
       " 'the',\n",
       " 'human',\n",
       " 'race',\n",
       " 'still',\n",
       " 'multi',\n",
       " '##pl',\n",
       " '##ies',\n",
       " 'like',\n",
       " 'bacteria',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pet',\n",
       " '##ri',\n",
       " 'dish',\n",
       " '.',\n",
       " 'classic',\n",
       " 'mal',\n",
       " '##thus',\n",
       " '##ian',\n",
       " 'economics',\n",
       " 'states',\n",
       " 'that',\n",
       " 'any',\n",
       " 'species',\n",
       " ',',\n",
       " 'including',\n",
       " 'humans',\n",
       " ',',\n",
       " 'will',\n",
       " 'multi',\n",
       " '##ply',\n",
       " 'beyond',\n",
       " 'their',\n",
       " 'means',\n",
       " 'of',\n",
       " 'subsistence',\n",
       " '.',\n",
       " '6',\n",
       " 'billion',\n",
       " 'and',\n",
       " 'growing',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'that',\n",
       " \"'\",\n",
       " 's',\n",
       " 'obscene',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of how the tokenizer transforms the text into a vector\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1676032112446,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "vuG95eEdbmLs",
    "outputId": "e537466d-efb4-452d-deee-92e12c3cebef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([101,\n",
       "  25176,\n",
       "  16136,\n",
       "  2665,\n",
       "  2003,\n",
       "  1037,\n",
       "  4438,\n",
       "  1012,\n",
       "  1045,\n",
       "  2031,\n",
       "  2042,\n",
       "  3403,\n",
       "  2005,\n",
       "  2619,\n",
       "  2000,\n",
       "  2128,\n",
       "  1011,\n",
       "  2079,\n",
       "  2009,\n",
       "  1012,\n",
       "  2027,\n",
       "  4025,\n",
       "  2000,\n",
       "  2022,\n",
       "  2128,\n",
       "  12614,\n",
       "  16596,\n",
       "  1011,\n",
       "  10882,\n",
       "  10002,\n",
       "  2122,\n",
       "  2420,\n",
       "  1006,\n",
       "  1045,\n",
       "  1012,\n",
       "  1041,\n",
       "  1012,\n",
       "  2162,\n",
       "  1997,\n",
       "  1996,\n",
       "  8484,\n",
       "  1007,\n",
       "  1998,\n",
       "  1045,\n",
       "  2572,\n",
       "  5327,\n",
       "  2070,\n",
       "  2472,\n",
       "  1013,\n",
       "  3135,\n",
       "  2097,\n",
       "  2128,\n",
       "  1011,\n",
       "  2079,\n",
       "  25176,\n",
       "  16136,\n",
       "  2665,\n",
       "  1012,\n",
       "  2007,\n",
       "  2651,\n",
       "  2015,\n",
       "  3274,\n",
       "  7284,\n",
       "  1998,\n",
       "  2974,\n",
       "  1010,\n",
       "  2009,\n",
       "  2052,\n",
       "  2031,\n",
       "  1996,\n",
       "  4022,\n",
       "  2000,\n",
       "  2022,\n",
       "  1037,\n",
       "  2307,\n",
       "  3861,\n",
       "  1012,\n",
       "  3424,\n",
       "  1011,\n",
       "  26425,\n",
       "  2078,\n",
       "  3152,\n",
       "  2089,\n",
       "  2025,\n",
       "  2022,\n",
       "  2008,\n",
       "  2521,\n",
       "  1011,\n",
       "  18584,\n",
       "  2098,\n",
       "  1012,\n",
       "  1996,\n",
       "  2529,\n",
       "  2679,\n",
       "  15910,\n",
       "  2066,\n",
       "  20997,\n",
       "  2229,\n",
       "  2007,\n",
       "  2053,\n",
       "  2648,\n",
       "  3747,\n",
       "  2000,\n",
       "  20099,\n",
       "  12502,\n",
       "  2009,\n",
       "  1012,\n",
       "  2057,\n",
       "  1010,\n",
       "  2004,\n",
       "  4286,\n",
       "  1010,\n",
       "  2031,\n",
       "  1996,\n",
       "  5724,\n",
       "  1997,\n",
       "  5128,\n",
       "  1996,\n",
       "  11382,\n",
       "  15853,\n",
       "  2232,\n",
       "  2006,\n",
       "  1996,\n",
       "  4013,\n",
       "  16748,\n",
       "  3370,\n",
       "  1997,\n",
       "  102],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of how the tokenizer transforms the vector containing strings into numbers\n",
    "#The numbers represent the position of the word in the vocab.txt file\n",
    "tokenizer.encode(text, max_len=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1676032112447,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "kb-nxXiycW5g",
    "outputId": "8ad931b9-f385-4ce8-e720-b5f3d82386af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'in', 'to']\n",
      "---------------------\n",
      "['from', 'her', '##s']\n"
     ]
    }
   ],
   "source": [
    "#Example of word and location in the vocab file\n",
    "print(list(token_dict)[1998:2001])\n",
    "print(\"---------------------\")\n",
    "print(list(token_dict)[2013:2016 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1676032112447,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "I1ckZ3csfU0b"
   },
   "outputs": [],
   "source": [
    "ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1676032112448,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "g0mT6LM4fctu",
    "outputId": "d9d2053a-e5c5-48d4-91bd-731597e9320a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John', 'Jenny'), ('Charles', 'Christy'), ('Mike', 'Monica')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is an example of how the function zip works. \n",
    "#This is useful to understand the function that processes the reviews.\n",
    "#The zip function forms pairs from the elements in two lists.\n",
    "\n",
    "a = (\"John\", \"Charles\", \"Mike\")\n",
    "b = (\"Jenny\", \"Christy\", \"Monica\")\n",
    "\n",
    "x = list(zip(a, b))\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1676032112448,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "wmNeg6oygZ43",
    "outputId": "e7f0b2ec-49e7-491d-d49b-8059a4c0c803"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mike', 'Monica'), ('John', 'Jenny'), ('Charles', 'Christy')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The function np.random.shuffle shuffles the order of the pairs\n",
    "\n",
    "np.random.shuffle(x)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1676032112448,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "MT7Y2FNFi-kS"
   },
   "outputs": [],
   "source": [
    "#When we use + with zip, we reverse the zipping process\n",
    "\n",
    "a,b = zip(*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1676032112449,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "P1wySEl6hDvx",
    "outputId": "8d01afa4-0f58-42d8-e1fd-9583e8c3e954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mike', 'John', 'Charles'], dtype='<U7')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " a = np.array(a)\n",
    " a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1676032112449,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "l0hsW6OnjW46",
    "outputId": "bdece74c-3741-43be-b939-f64a662baa5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is an example of how the % operator works. \n",
    "#This is useful to understand the function that processes the reviews.\n",
    "\n",
    "#The % operator is mostly to find the modulus of two integers. \n",
    "#a % b returns the remainder after dividing a by b\n",
    "\n",
    "9%2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2455,
     "status": "ok",
     "timestamp": 1676032114885,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "RL3ddLeN7z0g",
    "outputId": "d7a4a07f-e7bf-4146-8bbd-fb5ba37c55c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [00:02<00:00, 4475509.19it/s]\n"
     ]
    }
   ],
   "source": [
    "#This is an example of how the function tqdm works. \n",
    "#This is useful to understand the function that processes the reviews.\n",
    "\n",
    "for i in tqdm(range(0, int(1e7)), desc =\"Processing\"):\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 171586,
     "status": "ok",
     "timestamp": 1676032286463,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "JNAaF6Z-lMdi",
    "outputId": "caf5b96d-5df8-4dbc-88c0-1820216d76b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:44<00:00, 279.93it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:43<00:00, 288.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:41<00:00, 304.08it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12500/12500 [00:41<00:00, 298.26it/s]\n"
     ]
    }
   ],
   "source": [
    "#Final function to process all the reviews\n",
    "\n",
    "def load_data(path, tagset):\n",
    "    global tokenizer\n",
    "    indices, sentiments = [], []\n",
    "    for folder, sentiment in tagset:\n",
    "        folder = os.path.join(path, folder)\n",
    "        for name in tqdm(os.listdir(folder)):\n",
    "            with open(os.path.join(folder, name), 'r',encoding=\"utf-8\") as reader:\n",
    "                  text = reader.read()\n",
    "            ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "            indices.append(ids)\n",
    "            sentiments.append(sentiment)\n",
    "  \n",
    "    items = list(zip(indices, sentiments))\n",
    "    np.random.shuffle(items)\n",
    "    indices, sentiments = zip(*items)\n",
    "    indices = np.array(indices)\n",
    "    mod = indices.shape[0] % BATCH_SIZE\n",
    "    if mod > 0:\n",
    "        indices, sentiments = indices[:-mod], sentiments[:-mod]\n",
    "    return [indices, np.zeros_like(indices)], np.array(sentiments)\n",
    "  \n",
    "\n",
    "train_x, train_y = load_data(train_path, tagset)\n",
    "test_x, test_y = load_data(test_path, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1676032286464,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "RntXgBRrqz7C",
    "outputId": "d24d8daa-1488-4a2f-c302-8bfe98fadb2a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[  101,  7078, 10392, ...,  3496,  2000,   102],\n",
      "       [  101,  6583,  9905, ...,  7987,  1013,   102],\n",
      "       [  101,  1045,  1005, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,  1045,  2428, ...,  1010,  1000,   102],\n",
      "       [  101,  2065,  2017, ...,  2007,  1996,   102],\n",
      "       [  101,  1045,  2001, ...,  1999,  1996,   102]]), array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]])]\n",
      "------------------------------------------------------------------------\n",
      "[1 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#This is the output of train_x and train_y\n",
    "#train_x is an array of vector containing the numbers corresponding to the vocabulary.\n",
    "#train_y is a vector with zeros and ones. They represent a positive or negative review.\n",
    "print(train_x)\n",
    "print(\"------------------------------------------------------------------------\")\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building the model**\n",
    "\n",
    "<font size=\"3\">Here we are using the BERT model that we downloaded and put on our Google drive. Please remember that we loaded the keras_bert library and the function load_trained_model_from_checkpoint. We are loading BERT with the following script<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 124916,
     "status": "ok",
     "timestamp": 1676032411354,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "j22uTvDUners"
   },
   "outputs": [],
   "source": [
    "#Load model using the function load_trained_model_from_checkpoint from the keras_bert library\n",
    "model = load_trained_model_from_checkpoint(\n",
    "      config_path,\n",
    "      checkpoint_path,\n",
    "      training=True,\n",
    "      trainable=True,\n",
    "      seq_len=SEQ_LEN,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fine tunning**\n",
    "\n",
    "<font size=\"3\">For fine-tuning this model for classification tasks, we take the last layer NSP-Dense (Next Sentence Prediction-Dense) and tie its output to a new fully connected dense layer. In this example we take the last layer (NSP-Dense) and connect it to a binary classification layer. The binary classification layer is essentially a fully-connected dense layer with size 2. This is shown below<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1676032411354,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "jcmyhqrKnpta"
   },
   "outputs": [],
   "source": [
    "inputs = model.inputs[:2]\n",
    "dense = model.get_layer('NSP-Dense').output\n",
    "outputs = keras.layers.Dense(units=2, activation='softmax')(dense)\n",
    "model = keras.models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Compile new model**\n",
    "\n",
    "<font size=\"3\">We need to compile the new model. We will use Rectified Adam (RAdam) as the optimizer. The size of the last fully connected dense layer is equal to the number of classification classes or labels.\n",
    "The activation and loss function for binary and multiclass text classification is softmax. Since it is a case of binary classification, we want the probabilities of the output nodes to sum up to 1. That's why we use the softmax as the activation function. We also use the sparse categorical cross entropy loss function.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "ok",
     "timestamp": 1676032411355,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "zdrqYGMe10XI"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python import keras\n",
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1676032411355,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "PqewpmSS8m6N"
   },
   "outputs": [],
   "source": [
    " model.compile(\n",
    "        RAdam(lr=LR),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1676032412116,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "N0WISYm74WEv",
    "outputId": "a1551c4c-dd41-42c3-fd42-d8456be1478f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input-Token (InputLayer)       [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Input-Segment (InputLayer)     [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " Embedding-Token (TokenEmbeddin  [(None, 128, 768),  23440896    ['Input-Token[0][0]']            \n",
      " g)                              (30522, 768)]                                                    \n",
      "                                                                                                  \n",
      " Embedding-Segment (Embedding)  (None, 128, 768)     1536        ['Input-Segment[0][0]']          \n",
      "                                                                                                  \n",
      " Embedding-Token-Segment (Add)  (None, 128, 768)     0           ['Embedding-Token[0][0]',        \n",
      "                                                                  'Embedding-Segment[0][0]']      \n",
      "                                                                                                  \n",
      " Embedding-Position (PositionEm  (None, 128, 768)    98304       ['Embedding-Token-Segment[0][0]']\n",
      " bedding)                                                                                         \n",
      "                                                                                                  \n",
      " Embedding-Dropout (Dropout)    (None, 128, 768)     0           ['Embedding-Position[0][0]']     \n",
      "                                                                                                  \n",
      " Embedding-Norm (LayerNormaliza  (None, 128, 768)    1536        ['Embedding-Dropout[0][0]']      \n",
      " tion)                                                                                            \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Embedding-Norm[0][0]']         \n",
      " on (MultiHeadAttention)                                                                          \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Embedding-Norm[0][0]',         \n",
      " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-1-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-1-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-1-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-1-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-1-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-1-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-1-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-2-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-2-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-2-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-2-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-2-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-2-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-2-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-3-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-3-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-3-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-3-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-3-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-3-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-3-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-3-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-4-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-4-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-4-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-4-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-4-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-4-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-4-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-4-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-4-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-4-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-4-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-5-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-5-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-5-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-5-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-5-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-5-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-5-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-5-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-5-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-5-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-5-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-6-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-6-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-6-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-6-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-6-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-6-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-6-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-6-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-6-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-6-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-6-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-7-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-7-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-7-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-7-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-7-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-7-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-7-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-7-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-7-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-7-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-7-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-8-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-8-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-8-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-8-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-8-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-8-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-8-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-8-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-8-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    2362368     ['Encoder-8-FeedForward-Norm[0][0\n",
      " on (MultiHeadAttention)                                         ]']                              \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Dropout (Dropout)                                            n[0][0]']                        \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    0           ['Encoder-8-FeedForward-Norm[0][0\n",
      " on-Add (Add)                                                    ]',                              \n",
      "                                                                  'Encoder-9-MultiHeadSelfAttentio\n",
      "                                                                 n-Dropout[0][0]']                \n",
      "                                                                                                  \n",
      " Encoder-9-MultiHeadSelfAttenti  (None, 128, 768)    1536        ['Encoder-9-MultiHeadSelfAttentio\n",
      " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward (FeedFor  (None, 128, 768)    4722432     ['Encoder-9-MultiHeadSelfAttentio\n",
      " ward)                                                           n-Norm[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Dropout   (None, 128, 768)    0           ['Encoder-9-FeedForward[0][0]']  \n",
      " (Dropout)                                                                                        \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Add (Add  (None, 128, 768)    0           ['Encoder-9-MultiHeadSelfAttentio\n",
      " )                                                               n-Norm[0][0]',                   \n",
      "                                                                  'Encoder-9-FeedForward-Dropout[0\n",
      "                                                                 ][0]']                           \n",
      "                                                                                                  \n",
      " Encoder-9-FeedForward-Norm (La  (None, 128, 768)    1536        ['Encoder-9-FeedForward-Add[0][0]\n",
      " yerNormalization)                                               ']                               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion (MultiHeadAttention)                                        ]']                              \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-9-FeedForward-Norm[0][0\n",
      " ion-Add (Add)                                                   ]',                              \n",
      "                                                                  'Encoder-10-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-10-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-10-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-10-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-10-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-10-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-10-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-10-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-10-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-10-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-10-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-11-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-11-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-11-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-11-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-11-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-11-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-11-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-11-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-11-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    2362368     ['Encoder-11-FeedForward-Norm[0][\n",
      " ion (MultiHeadAttention)                                        0]']                             \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    0           ['Encoder-11-FeedForward-Norm[0][\n",
      " ion-Add (Add)                                                   0]',                             \n",
      "                                                                  'Encoder-12-MultiHeadSelfAttenti\n",
      "                                                                 on-Dropout[0][0]']               \n",
      "                                                                                                  \n",
      " Encoder-12-MultiHeadSelfAttent  (None, 128, 768)    1536        ['Encoder-12-MultiHeadSelfAttenti\n",
      " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward (FeedFo  (None, 128, 768)    4722432     ['Encoder-12-MultiHeadSelfAttenti\n",
      " rward)                                                          on-Norm[0][0]']                  \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Dropout  (None, 128, 768)    0           ['Encoder-12-FeedForward[0][0]'] \n",
      "  (Dropout)                                                                                       \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Add (Ad  (None, 128, 768)    0           ['Encoder-12-MultiHeadSelfAttenti\n",
      " d)                                                              on-Norm[0][0]',                  \n",
      "                                                                  'Encoder-12-FeedForward-Dropout[\n",
      "                                                                 0][0]']                          \n",
      "                                                                                                  \n",
      " Encoder-12-FeedForward-Norm (L  (None, 128, 768)    1536        ['Encoder-12-FeedForward-Add[0][0\n",
      " ayerNormalization)                                              ]']                              \n",
      "                                                                                                  \n",
      " Extract (Extract)              (None, 768)          0           ['Encoder-12-FeedForward-Norm[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " NSP-Dense (Dense)              (None, 768)          590592      ['Extract[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20)           15380       ['NSP-Dense[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,202,708\n",
      "Trainable params: 109,202,708\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fit the model**\n",
    "\n",
    "<font size=\"3\">The next setp is to fit the model. We will train it using 3 epochs and a batch size of 16. It might take a little less than 1 hour using the Colab GPU.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2112099,
     "status": "ok",
     "timestamp": 1676034524203,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "iq96dfIM4eci",
    "outputId": "10143d27-5684-463e-d46b-7072b1aa444b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1250/1250 [==============================] - 743s 551ms/step - loss: 0.5274 - sparse_categorical_accuracy: 0.7926 - val_loss: 0.2855 - val_sparse_categorical_accuracy: 0.8774\n",
      "Epoch 2/3\n",
      "1250/1250 [==============================] - 685s 548ms/step - loss: 0.2461 - sparse_categorical_accuracy: 0.9006 - val_loss: 0.2837 - val_sparse_categorical_accuracy: 0.8782\n",
      "Epoch 3/3\n",
      "1250/1250 [==============================] - 684s 547ms/step - loss: 0.1396 - sparse_categorical_accuracy: 0.9478 - val_loss: 0.3743 - val_sparse_categorical_accuracy: 0.8686\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=0.20,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluate the results**\n",
    "\n",
    "<font size=\"3\">We can calculate the accuracy of the model using the test database we uploaded before. In this example we got pretty good results, close to 86% accuracy<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 267401,
     "status": "ok",
     "timestamp": 1676034791595,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "w9DKCVcnR_1q",
    "outputId": "620a1728-052c-47fe-f204-a3a31ea18876",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 226s 285ms/step\n",
      "Accuracy: 0.8641165172855314\n",
      "macro_f1: 0.8636751784892873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "predicts = model.predict(test_x, verbose=True).argmax(axis=-1)\n",
    "accuracy = accuracy_score(test_y, predicts)\n",
    "macro_f1 = f1_score(test_y, predicts, average='macro')\n",
    "print (\"Accuracy: %s\" % accuracy)\n",
    "print (\"macro_f1: %s\" % macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Perform sentiment analysis on new prompts**\n",
    "\n",
    "<font size=\"3\">Here we have a list of 5 short reviews. Using the function predict we can observe if the model assigns a positive or negative review to the statement.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4445,
     "status": "ok",
     "timestamp": 1676034796004,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "3f0CyWVdSA9h",
    "outputId": "a19cb874-f08e-4aad-bfb7-8f17ad488a90",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "positive: It's a must watch\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "positive: Can't wait for it's next part!\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "negative: It fell short of expectations.\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "negative: Wish there was more to it!\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "positive: Just wow!\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "negative: Colossial waste of time\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "negative: Save youself from this 90 mins trauma!\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "  \"It's a must watch\",\n",
    "  \"Can't wait for it's next part!\",\n",
    "  'It fell short of expectations.',\n",
    "  'Wish there was more to it!',\n",
    "  'Just wow!',\n",
    "  'Colossial waste of time',\n",
    "  'Save youself from this 90 mins trauma!'\n",
    "]\n",
    "for text in texts:\n",
    "  ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "  inpu = np.array(ids).reshape([1, SEQ_LEN])\n",
    "  predicted_id = model.predict([inpu,np.zeros_like(inpu)]).argmax(axis=-1)[0]\n",
    "  print (\"%s: %s\"% (id_to_labels[predicted_id], text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Perform sentiment analysis on new prompts from another domain**\n",
    "\n",
    "<font size=\"3\">Now we will try the sentiment analysis task not on movie reviews but on statements given by Chief Financial Officers on their earning calls conferences. Here we selected only 2 of them. The second statement talked about losses and it should be labeled as a negative statement. This opens the question on wheter we should considering running a domain adaptation process for the model. That is basically train the model on new words that are frequent on the financial jargon. We will explore that in another notebook.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1676034796005,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "krIqtVOIpEp3",
    "outputId": "a31080db-239a-4473-efe0-2c38e953cd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n",
      "positive: Yes. So we've never really disclosed beds per door, anything like that.  What I will say is, we actually just completed a pretty big deep dive on this with cohort  views. And no matter how we cut it, we are continuing to see same-store sales increase,  which is terrific , and Q4 was no exception to that. So our strength in the marketplace continues to grow\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "positive: Understood.I'd say that we probably lost $0.5 million to $0.75 million in the fourth quarter of the year due to  some of those headwinds as an approximation for the combination of outages , weathers and the like.\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "  \"Yes. So we've never really disclosed beds per door, anything like that.\\\n",
    "  What I will say is, we actually just completed a pretty big deep dive on this with cohort\\\n",
    "  views. And no matter how we cut it, we are continuing to see same-store sales increase,\\\n",
    "  which is terrific , and Q4 was no exception to that. So our strength in the marketplace continues to grow\",  \n",
    "  \"Understood.I'd say that we probably lost $0.5 million to $0.75 million in the fourth quarter of the year due to\\\n",
    "  some of those headwinds as an approximation for the combination of outages , weathers and the like.\"\n",
    "]\n",
    "for text in texts:\n",
    "  ids, segments = tokenizer.encode(text, max_len=SEQ_LEN)\n",
    "  inpu = np.array(ids).reshape([1, SEQ_LEN])\n",
    "  predicted_id = model.predict([inpu,np.zeros_like(inpu)]).argmax(axis=-1)[0]\n",
    "  print (\"%s: %s\"% (id_to_labels[predicted_id], text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1676034796006,
     "user": {
      "displayName": "ANDRES GOMEZ",
      "userId": "04080239744905773320"
     },
     "user_tz": 300
    },
    "id": "7ES1vQgrSHIM"
   },
   "source": [
    "> *What we have to learn to do, we learn by doing*. *Aristotle*\n",
    "\n",
    "<font size=\"3\">\n",
    "Follow me on <a href=\"https://co.linkedin.com/in/andres-gomez-hernandez\" target=\"_blank\">Linkedin</a> for topics about quantitative finance, data science and emerging markets.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMJi+4QeyI5C1w7biqMjFYx",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
